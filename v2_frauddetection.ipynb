{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys;\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() \n",
    "region = session.boto_region_name \n",
    "\n",
    "bucket = 'sagemaker-jfox'\n",
    " \n",
    "prefix = 'sagemaker/xgboost'\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f'Bucket {bucket} in region {region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import pandas as pd\n",
    "raw_data_filename = 'frauddetection.csv'\n",
    "\n",
    "s3 = boto3.resource('s3', region_name=region)\n",
    "s3.Bucket(bucket).download_file('frauddetection/'+raw_data_filename, raw_data_filename)\n",
    "percent_to_read = 100\n",
    "fraction_to_read=percent_to_read/100 # \n",
    "df = pd.read_csv('./'+raw_data_filename,  skiprows=lambda i: i>0 and random.random() > fraction_to_read)\n",
    "pd.set_option('display.max_rows', 10) \n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print('Length', len(df))\n",
    "target_col='isFraud'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `isFraud` rows have `type` `TRANSFER` or `CASH_OUT`, never but `CASH_IN` or `PAYMENT`. Could filter these out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts of each class to determine imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def count_positive_and_negative(df):\n",
    "    num_positive = len(df.loc[  df[target_col] == 1 ])\n",
    "    num_negative = len(df) - num_positive\n",
    "    return num_positive, num_negative\n",
    "\n",
    "num_positive, num_negative = count_positive_and_negative(df)\n",
    "\n",
    "print('Fraud', num_positive, '; Not fraud', num_negative, '; Total', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using `isFlaggedFraud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['isFlaggedFraud'], axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution of positive vs negative, in log scale because of the imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt   \n",
    "def plot_positive_negative_counts(df, target_col):\n",
    "    val_counts=df[target_col].value_counts()\n",
    "    fig, ax = plt.subplots()\n",
    "   #ax.set(yscale=\"log\")\n",
    "    plt.bar(['No', 'Yes'], val_counts)\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "plot_positive_negative_counts(df,target_col)\n",
    "num_positive, num_negative = count_positive_and_negative(df)\n",
    "\n",
    "print('Not fraud', num_negative, '; fraud', num_positive, '; Total', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df[df.isFraud==1]\n",
    "df_pos = df_pos.copy(deep=True)\n",
    "for _ in range(300):\n",
    "   df=pd.concat([df,df_pos.copy(deep=False)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positive_negative_counts(df,target_col)\n",
    "num_positive, num_negative = count_positive_and_negative(df)\n",
    "\n",
    "print('Not fraud', num_negative, '; fraud', num_positive, '; Total', len(df))\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split numerical from non-numerical\n",
    "columns = df.columns\n",
    "numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "other_col =[c for c in columns if  c not in numerical_cols]\n",
    "df_other = df[other_col]\n",
    "df_num = df[numerical_cols]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_num_std = pd.DataFrame(scaler.fit_transform(df_num), columns=df_num.columns)\n",
    " \n",
    "df2 = pd.concat([df_num_std, df_other], axis=1)\n",
    "df2 = df[columns] # Put back in old order\n",
    "df=df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dummies (onehot) for `type` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols.remove(target_col)\n",
    "cols = [target_col] + cols\n",
    "df = df[cols] # Move target to the left\n",
    "\n",
    "df_dummies=pd.get_dummies(df['type'],drop_first=True )\n",
    "\n",
    "df=df.drop(['type'], axis=1)\n",
    "df = pd.concat([df, df_dummies], axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use *HashingEncoder* to handle categorical columns with high cardinality. These cannot be onehotted as that would generate too many columns and a too-sparse matrix. However, this seems to provide no accuracy benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "high_cardinality__categorical_col = ['nameOrig', 'nameDest']\n",
    "use_hashing_encoder = False\n",
    "if not use_hashing_encoder:\n",
    "    print('Dropping high-cardinality categorical columns')\n",
    "    df=df.drop(high_cardinality__categorical_col, axis=1)\n",
    "else:\n",
    "  import category_encoders as ce  \n",
    "  columns_before = [x for x in df.columns if x not in high_cardinality__categorical_col+[target_col]]\n",
    "\n",
    "  def make_col_mapping(cols):\n",
    "    col_mapping = {}\n",
    "    for c in cols:\n",
    "        if c[:4]=='col_':\n",
    "          num = c.split('_')[-1]\n",
    "          int(num) # check format\n",
    "          col_mapping[c] = hashencode_this + \"_\" + num\n",
    "     \n",
    "    return col_mapping\n",
    "\n",
    "  \n",
    "  def hashencode(hashencode_this, df, previous_hash_cols):\n",
    "    \"\"\"Could hashencode both columns, but for now hashencode does not seem to provide a benefit.\"\"\"\n",
    "    for c in df.columns:\n",
    "        assert c[:4]!=\"col_\",  df.columns\n",
    "    cpus_in_t2xlarge = 4\n",
    "    default_max_sample = len(df)/cpus_in_t2xlarge\n",
    "    max_sample = default_max_sample/2\n",
    "    ce_hash = ce.HashingEncoder(cols = [hashencode_this],max_sample=max_sample)\n",
    "    X1 = df.drop([target_col], axis=1)\n",
    "    y1 = df[target_col]\n",
    "    with_hashing = ce_hash.fit_transform(X1, y1)\n",
    "    hashed = with_hashing.drop(columns_before+previous_hash_cols, axis=1)\n",
    "    generated_cols = [x for x in hashed.columns if x[:4]=='col_']\n",
    "    col_mapping = make_col_mapping(generated_cols)\n",
    "    \n",
    "    hashed = hashed.rename(columns = col_mapping)\n",
    "    df = pd.concat([y1, X1, hashed], axis=1)\n",
    " \n",
    "    df = df.drop([hashencode_this], axis=1)\n",
    "    return df, list(col_mapping.values())\n",
    " \n",
    "  previous_hash_cols = []\n",
    "  for hashencode_this in high_cardinality__categorical_col: \n",
    "   df, previous_hash_cols = hashencode(hashencode_this,df,previous_hash_cols)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally use *SMOTENC* for unbalanced classes, though we may stick with the XGBoost parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "# Using weighting in XGBOOST instead of SMOTENC\n",
    "use_smote = False\n",
    "if use_smote:\n",
    "   ycol=target_col\n",
    "   Xcol=list(df.columns)\n",
    "   Xcol.remove(ycol)\n",
    " \n",
    "   categorical_columns=[i for i in range(len(Xcol)) \n",
    "                     if Xcol[i] not in ['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']]\n",
    "\n",
    "   smotenc = SMOTENC(categorical_columns,random_state = 101)\n",
    "\n",
    "   X, y = smotenc.fit_resample(df[Xcol], df[ycol])\n",
    "   y_df = pd.DataFrame({target_col: y} )\n",
    "\n",
    "   df = pd.concat([X, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positive_negative_counts(df,target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "len_=len(df)\n",
    "train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len_), int(0.9 * len_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using libSVM for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.datasets import dump_svmlight_file   \n",
    "\n",
    "lengths = [] \n",
    "\n",
    "for d in [(train_data, 'train.libsvm'), ( validation_data, 'validation.libsvm'), (test_data, 'test.libsvm')]:\n",
    "   dataset=d[0]\n",
    "   file_ = d[1]\n",
    "   lengths.append((d[1].split('.')[0],len(dataset)))\n",
    "   dump_svmlight_file(X=dataset.drop([target_col], axis=1), y=dataset[target_col], f=d[1])\n",
    "\n",
    "print('Length of datasets:', lengths )\n",
    "\n",
    "s3 = boto3.resource('s3', region_name=region)#TODO Remove\n",
    "\n",
    "for filename in ['train.libsvm', 'validation.libsvm']:\n",
    "   s3.Bucket(bucket).Object(prefix + '/'+filename.split('.')[0]+'/'+filename).upload_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_path(subset):\n",
    "  return sagemaker.TrainingInput(s3_data='s3://{}/{}/{}'.format(bucket, prefix,subset), content_type='libsvm')\n",
    "\n",
    "s3_input_train = s3_path('train')\n",
    "s3_input_validation =s3_path('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Our data is now ready to be used to train a XGBoost model. The XGBoost algorithm has many tunable hyperparameters. Some of these hyperparameters are listed below; initially we'll only use a few of them.  \n",
    "\n",
    "- `max_depth`: Maximum depth of a tree. As a cautionary note, a value too small could underfit the data, while increasing it will make the model more complex and thus more likely to overfit the data (in other words, the classic bias-variance tradeoff).\n",
    "- `eta`: Step size shrinkage used in updates to prevent overfitting.  \n",
    "- `eval_metric`: Evaluation metric(s) for validation data. For data sets such as this one with imbalanced classes, we'll use the AUC metric.\n",
    "- `scale_pos_weight`: Controls the balance of positive and negative weights, again useful for data sets having imbalanced classes.\n",
    "\n",
    "First we'll set up the parameters for an Amazon SageMaker Estimator object, and the hyperparameters for the algorithm itself.  The Estimator object from the Amazon SageMaker Python SDK is a convenient way to set up training jobs with a minimal amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(region, 'xgboost','1.0-1')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    base_job_name='fraud-detection-job',\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "num_positive, num_negative=count_positive_and_negative(train_data)\n",
    "scale_pos_weight= 1 if not num_positive else num_negative / num_positive  \n",
    "\n",
    "print(f'num_positive: {num_positive}, num_negative: {num_negative}, scale_pos_weight {scale_pos_weight:.1f}')\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=3,\n",
    "                        eta=0.1,\n",
    "                        subsample=0.5,\n",
    "                        eval_metric='aucpr',\n",
    "                        objective='binary:logistic',\n",
    "                        scale_pos_weight=scale_pos_weight,\n",
    "                        num_round=110)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hosted training (`fit`) job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the XGBoost algorithm on our data, we can deploy the trained model to an Amazon SageMaker hosted endpoint with one simple line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import time\n",
    "from  sagemaker.serializers import LibSVMSerializer\n",
    "from  sagemaker.deserializers import StringDeserializer\n",
    "serializer = LibSVMSerializer()\n",
    "deserializer = StringDeserializer() \n",
    "deserializer.CONTENT_TYPE = \"text/libsvm\"\n",
    "\n",
    "model = xgb.create_model()\n",
    "\n",
    "container_def = model.prepare_container_def(instance_type='ml.m4.xlarge')\n",
    "\n",
    "\n",
    "def date_s():\n",
    " return     datetime.datetime.now().isoformat().replace('.','-').replace(':','-')[:-7] +'Z' \n",
    "\n",
    "model_name = 'fraud-' +date_s()\n",
    "print('model_name', model_name)\n",
    " \n",
    "endpoint_name = 'fraud-detection-endpoint'\n",
    " \n",
    "def deploy_on_new_endpoint():\n",
    "      print('deploy on new endpoint')\n",
    "      ret_unused_because_no_ser_deser = xgb.deploy(endpoint_name=endpoint_name, model_name=model_name,\n",
    "                                 initial_instance_count=1, instance_type='ml.m5.xlarge', accelerator_type='ml.eia2.medium')\n",
    "      ret = sagemaker.predictor.Predictor(endpoint_name=endpoint_name, \n",
    "                                              sagemaker_session=sagemaker.Session(),\n",
    "                                              serializer=serializer,\n",
    "                                              deserializer = deserializer\n",
    "                                             )\n",
    "  \n",
    "\n",
    "      return ret\n",
    "    \n",
    "def deploy_on_existing_endpoint():\n",
    "        print('deploy on existing endpoint') \n",
    "        session.create_model(model_name, role, container_def)\n",
    "        \n",
    "        endpoint_config_name = session.create_endpoint_config(name=model_name+date_s(),\n",
    "                                                      model_name=model_name,\n",
    "                                                      initial_instance_count=1,\n",
    "                                                      instance_type='ml.m5.xlarge',\n",
    "                                                      accelerator_type='ml.eia2.medium')\n",
    "        \n",
    "        client = boto3.client('sagemaker')\n",
    "        updated_endpoint=client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "     \n",
    "\n",
    "        ret = sagemaker.predictor.Predictor(endpoint_name=endpoint_name, \n",
    "                                              sagemaker_session=sagemaker.Session(),\n",
    "                                              serializer=serializer,\n",
    "                                              deserializer = deserializer\n",
    "                                             )\n",
    " \n",
    "        return ret\n",
    " \n",
    " \n",
    "while True:\n",
    "  try:\n",
    "     xgb_predictor= deploy_on_existing_endpoint()\n",
    "     break\n",
    "  except Exception as e:\n",
    "      if 'Cannot update in-progress endpoint' in str(e):\n",
    "        print('Cannot update in-progress endpoint')\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "      elif 'Could not find endpoint' in str(e):\n",
    "        print('Could not find endpoint')\n",
    "        xgb_predictor= deploy_on_new_endpoint()\n",
    "        break;\n",
    "      else:\n",
    "        print(\"Exception\", e)\n",
    "        break\n",
    "      \n",
    "print('finished deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate predictions from  the  test data set.\n",
    "\n",
    "Compared actual to predicted values of whether the transaction was a \"fraud\" (`1`) or not (`0`).  Then we'll produce a  confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (datetime.datetime.now())\n",
    "def do_predict(data):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = xgb_predictor.predict(payload) \n",
    "    result = response.split(',')\n",
    "    preds = [float((num)) for num in result]\n",
    "    preds = [round(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "def batch_predict(data, batch_size):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    \n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset+batch_size < items:\n",
    "            results = do_predict(data[offset:(offset+batch_size)])\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items]))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "with open('test.libsvm', 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [int(line.split(' ')[0]) for line in payload.split('\\n')]\n",
    "test_data = [line for line in payload.split('\\n')]\n",
    "preds = batch_predict(test_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actual_pos = sum(1 for i in range(len(labels)) if 1==labels[i])\n",
    "actual_neg = sum(1 for i in range(len(labels)) if 0==labels[i])\n",
    "\n",
    "pred_pos = sum(1 for i in range(len(preds)) if 1==preds[i])\n",
    "pred_neg = sum(1 for i in range(len(preds)) if 0==preds[i])\n",
    "\n",
    "true_pos = sum(1 for i in range(len(preds)) if preds[i]==1==labels[i])\n",
    "true_neg = sum(1 for i in range(len(preds)) if preds[i]==0==labels[i])\n",
    "\n",
    "false_pos=sum(1 for i in range(len(preds)) if preds[i]==1 and 0==labels[i])\n",
    "false_neg=sum(1 for i in range(len(preds)) if preds[i]==0 and 1==labels[i])\n",
    "\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "precision = true_pos/(true_pos+false_pos)\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "error = sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))\n",
    "assert recall <= f1 <= precision or precision <= f1 <= recall \n",
    "assert true_pos + false_neg == actual_pos\n",
    "assert true_neg + false_pos == actual_neg\n",
    "#print('pred_pos', pred_pos,'pred_neg', pred_neg)\n",
    "\n",
    "assert len(preds)==len(labels)\n",
    "\n",
    "#print(len(preds), 'predictions and labels')\n",
    "\n",
    "#print('actual_pos', actual_pos, 'actual_neg', actual_neg)\n",
    "assert actual_pos+actual_neg==len(labels)\n",
    "                   \n",
    "#print('true_pos', true_pos, 'false_pos', false_pos)\n",
    "#assert  true_pos+false_pos == pred_pos\n",
    "\n",
    "#print('true_neg', true_neg, 'false_neg', false_neg )\n",
    "assert true_neg+false_neg== pred_neg\n",
    "\n",
    "#print('pred_pos+pred_neg',pred_pos+pred_neg)\n",
    "assert pred_pos+pred_neg==len(preds)\n",
    "\n",
    "print(f'Recall={recall:.2f}')\n",
    "print(f'Precision={precision:.2f}')\n",
    "print(f'Error rate={error:.2f}')\n",
    "print(f'F1={f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=np.array(labels), columns=np.array(preds), rownames=['actual fraud'],  colnames=['predicted as fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up to save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
