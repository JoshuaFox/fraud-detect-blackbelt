{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import datetime\n",
    "\n",
    "print('Starting at', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "!{sys.executable} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() \n",
    "region = session.boto_region_name \n",
    "\n",
    "bucket = 'sagemaker-jfox'\n",
    "algo =  'linearlearner'\n",
    "prefix = 'sagemaker/' + algo\n",
    "role = sagemaker.get_execution_role()\n",
    "print('Role', role)\n",
    "print(f'Bucket {bucket} in region {region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os \n",
    "\n",
    "raw_data_filename = 'frauddetection.parquet.snappy'\n",
    " \n",
    "s3 = boto3.resource('s3', region_name=region)\n",
    "\n",
    "s3_path_snappy = 'prepareddataalltypes/datapreparation_20Nov2020_1605858525820/datapreparation_20Nov2020_1605858525820_part00000.parquet.snappy'\n",
    "\n",
    "if not os.path.isfile(raw_data_filename):\n",
    "   s3.Bucket(bucket).download_file(s3_path_snappy,  raw_data_filename)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "table = pq.read_table(raw_data_filename)\n",
    "\n",
    "df = table.to_pandas()\n",
    "\n",
    "pd.set_option('display.max_rows', 10) \n",
    " \n",
    "print(len(df), 'data items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Fraudulent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df = df.rename(columns = {'isFraud': target_col,\n",
    "                          'type_CASH_OUT': 'type_CashOut',\n",
    "                          'type_TRANSFER': 'type_Transfer',\n",
    "                          'type_CASH_IN': 'type_CashIn',\n",
    "                          'type_PAYMENT': 'type_Payment',\n",
    "                          'oldbalanceOrg_nml': 'originOldBalance',\n",
    "                          'oldBalanceOrigSign': 'originOldBalanceSign', \n",
    "                          'newbalanceOrig_nml': 'originNewBalance',\n",
    "                          'negDeltaOrigin_nml': 'negativeDeltaInOrigin', \n",
    "                          'oldbalanceDest_nml': 'destinationOldBalance',\n",
    "                          'newbalanceDest_nml': 'destinationNewBalance',\n",
    "                          'expectedNewBalDest_nml': 'destinationExpectedNewBalance', # based on old value and delta   \n",
    "                          'amount_nml': 'amount',\n",
    "                          'hourOf24_radians': 'hourOf24_radians',# To be deleted after calculation\n",
    "                          'isFlaggedFraud':'FlaggedFraud'#To be deleted\n",
    "                         })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`type_X` columns where X is one of  `PAYMENT` `TRANSFER` `CASH_OUT` `DEBIT` `CASH_IN`.\n",
    "There was also `DEBIT`, but onehotting drops one of the types because of correlation.\n",
    "All `isFraud` rows have `type` `TRANSFER` or `CASH_OUT`, never  `CASH_IN` or `PAYMENT` or `DEBIT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts of each class to determine imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive_and_negative(df):\n",
    "    num_positive = len(df.loc[  df[target_col] == 1 ])\n",
    "    num_negative = len(df) - num_positive\n",
    "    return num_positive, num_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[(df.type_Transfer==1) | (df.type_CashOut==1 ) ]\n",
    "print('Filtered from', f'{len(df):,}', 'items to', f'{len(df_filtered):,}')\n",
    "num_positive, num_negative = count_positive_and_negative(df)\n",
    "print('Uniltered: Fraud', num_positive, '; Not fraud', num_negative, '; Total', len(df))\n",
    "num_positive, num_negative = count_positive_and_negative(df_filtered)\n",
    "print('Filtered: Fraud', num_positive, '; Not fraud', num_negative, '; Total', len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that target is on the left, per Sagemaker standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert target_col == df.columns.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform hour of day to a form suited for cyclical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "hours_col = 'hourOf24_radians'\n",
    "df['sineHourInDay'] = np.sin(df[hours_col])\n",
    "df['cosineHourInDay'] = np.cos(df[hours_col])\n",
    "df = df.drop(hours_col, axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots( 1,1, figsize=(11,9))\n",
    "\n",
    "def heatmap_all_features():\n",
    "    \n",
    "    lbl='Fraud label against all features'\n",
    "    axes .set_title(f'{lbl}: Heatmap')\n",
    "    seaborn.heatmap(df.corr(),  cmap='RdPu', ax=axes)\n",
    "    \n",
    "heatmap_all_features()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn  \n",
    "\n",
    "fig, axes = plt.subplots( 1,2, figsize=(15,5))\n",
    "\n",
    "def heatmap(fraud_cases):\n",
    "    \n",
    "    lbl='fraudulent' if fraud_cases else 'non-fraudulent'\n",
    "    axes[1-fraud_cases].set_title(f'Only {lbl} cases: Heatmap')\n",
    "    seaborn.heatmap(df.query(f'{target_col} == {fraud_cases}').drop([target_col], 1).corr(),  cmap='Oranges', ax=axes[1-fraud_cases])\n",
    "    \n",
    "heatmap(True)\n",
    "heatmap(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution of positive vs negative, in log scale because of the imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "def plot_positive_negative_counts(df, target_col):\n",
    "        df['fraudyn']=np.where(df[target_col]==1, 'Yes', 'No')\n",
    "        df['fraudyn'].value_counts().plot.pie(figsize=(5, 5), autopct='%.1f%%')\n",
    "        plt.title('Non-fraud vs fraud')\n",
    "        plt.tight_layout()\n",
    "        del df['fraudyn']\n",
    "    \n",
    "plot_positive_negative_counts(df,target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df[df.Fraudulent==1]\n",
    "df_neg = df[df.Fraudulent==0]\n",
    "len_df_pos = len(df_pos)\n",
    "ratio = len(df_neg)/ len_df_pos\n",
    "limit_oversampling = 5\n",
    "multiply_positives = int(ratio / limit_oversampling)\n",
    "\n",
    "df_pos_new = df_pos.copy() \n",
    "for _ in range(multiply_positives-1):   \n",
    "   df_pos_new = pd.concat([df_pos_new, df_pos.copy()], axis=0, ignore_index=True )\n",
    "    \n",
    " \n",
    "df2 = pd.concat([df_pos_new,df_neg], axis=0 )\n",
    "print(f'\\nAfter oversampling, new dataframe of length {len(df2):,} has {len(df_pos_new):,} positives instead of  {len_df_pos:,} positives (also: {len(df_neg):,} negatives)')\n",
    "\n",
    "df = df2\n",
    "\n",
    "plot_positive_negative_counts(df,target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle (and potentially take a fractional sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction=0.3 # Fraction to sample\n",
    "df = df.sample(frac=fraction).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split  with randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "\n",
    "rand_split = np.random.rand(len(df))\n",
    " \n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = (rand_split >= 0.9)\n",
    " \n",
    "data_train = df[train_list]\n",
    "data_val = df[val_list]\n",
    "data_test = df[test_list]\n",
    "\n",
    "\n",
    "print(f'Data: Total: {len(df):,d}; ' +\n",
    "      f'Training {len(data_train):,d}; ' + \n",
    "      f'Validation: {len(data_val):,d}; ' +\n",
    "      f'Testing: {len(data_test):,d}')\n",
    "\n",
    "train_y = ( data_train.iloc[:,1]  ).to_numpy();\n",
    "train_X = data_train.iloc[:,2:].to_numpy();\n",
    "\n",
    "val_y = ( data_val.iloc[:,1]).to_numpy();\n",
    "val_X = data_val.iloc[:,2:].to_numpy();\n",
    "\n",
    "test_y =  (data_test.iloc[:,1]).to_numpy();\n",
    "test_X = data_test.iloc[:,2:].to_numpy();\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'linear_train.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "f.seek(0)\n",
    " \n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = 'linear_validation.data'\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "f.seek(0)\n",
    " \n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', validation_file)).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_training_params = {\n",
    "    'RoleArn': role,\n",
    "    'TrainingJobName': 'Fill in this value before using the params',\n",
    "    'AlgorithmSpecification': {\n",
    "        'TrainingImage': container,\n",
    "        'TrainingInputMode': 'File'\n",
    "         \n",
    "    },\n",
    "    'ResourceConfig': {\n",
    "        'InstanceCount': 8,\n",
    "        'InstanceType': 'ml.c4.2xlarge',\n",
    "        'VolumeSizeInGB': 10\n",
    "    },\n",
    "    'InputDataConfig': [\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}/{}/train/'.format(bucket, prefix),\n",
    "                    'S3DataDistributionType': 'ShardedByS3Key'\n",
    "                }\n",
    "            },\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None'\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'validation',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}/{}/validation/'.format(bucket, prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None'\n",
    "        }\n",
    "\n",
    "    ],\n",
    "    'OutputDataConfig': {\n",
    "        'S3OutputPath': 's3://{}/{}/'.format(bucket, prefix)\n",
    "    },\n",
    "    'HyperParameters': {\n",
    "        'feature_dim': '-999999',# Fill in this value before using the params\n",
    "        'mini_batch_size': '4',\n",
    "        'binary_classifier_model_selection_criteria': 'f1',\n",
    "        'predictor_type': 'binary_classifier',\n",
    "        'epochs': '5',\n",
    "        'num_models': '32',\n",
    "        'loss': 'absolute_loss'\n",
    "    },\n",
    "    'StoppingCondition': {\n",
    "        'MaxRuntimeInSeconds':  59 * 60,\n",
    "        'MaxWaitTimeInSeconds': 60 * 60\n",
    "\n",
    "    },\n",
    "    'EnableManagedSpotTraining': True,    \n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linear_job = 'linear-' + time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "print('Job name', linear_job)\n",
    "\n",
    "linear_training_params['TrainingJobName'] = linear_job\n",
    "feature_dim = len(df.columns)-2 # Why is -2 needed to match the error-message's determination of feature dim?\n",
    "linear_training_params['HyperParameters']['feature_dim'] = str(feature_dim)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "sm.create_training_job(**linear_training_params)\n",
    "counter = 0\n",
    "status = 'InProgress'\n",
    "while status == 'InProgress':\n",
    "    status = sm.describe_training_job(TrainingJobName=linear_job)['TrainingJobStatus']\n",
    "    print(status,counter,'min',end='; ')\n",
    "    time.sleep(30)\n",
    "    counter += 0.5\n",
    "\n",
    "err_message = sm.describe_training_job(TrainingJobName=linear_job).get('FailureReason')\n",
    "if err_message:\n",
    "    print('Failure reason if any: {}'.format(err_message))\n",
    "    \n",
    "print('\\n\\nDone:', status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Deploy the trained model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import time\n",
    "\n",
    "endpoint_name = 'fraud-detect-endpoint'\n",
    "training_job_name=linear_job\n",
    "model_name = training_job_name + '-model'\n",
    "\n",
    "info = session.describe_training_job(training_job_name)\n",
    "\n",
    "model_data = info.get('ModelArtifacts',{}).get('S3ModelArtifacts')\n",
    "if model_data:\n",
    " print(model_data)\n",
    "else: print(info)\n",
    "\n",
    "primary_container = {\n",
    "  'Image': container,\n",
    "  'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = session.create_model(\n",
    "      name = model_name,\n",
    "      role = role,\n",
    "      container_defs = primary_container)\n",
    "\n",
    "\n",
    "endpoint_config_name = session.create_endpoint_config(name=endpoint_name + '-config-' + \n",
    "                                              time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime()),\n",
    "                                              model_name=model_name,\n",
    "                                              initial_instance_count=1,\n",
    "                                              instance_type='ml.m5.xlarge',\n",
    "                                              accelerator_type='ml.eia2.medium')\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "minutes_passed = 0\n",
    "try_now = True\n",
    "while try_now:\n",
    "  minutes_passed += 0.5\n",
    "  try:\n",
    "     updated_endpoint = client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "     try_now = False\n",
    "  except Exception as e:\n",
    "     if 'Could not find endpoint'  in str(e):\n",
    "        print('Could not find endpoint; Trying to create')\n",
    "        updated_endpoint=client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "        try_now = False\n",
    "     elif 'Cannot update in-progress endpoint' in str(e):\n",
    "        print(f'{minutes_passed} min: Cannot update in-progress endpoint; retrying', end='. ')\n",
    "        time.sleep(30)\n",
    "        try_now = True\n",
    "     else:  \n",
    "        raise e\n",
    "\n",
    "endpt_status = 'Updating'\n",
    "minutes_passed = 0 \n",
    "while endpt_status in ['Updating', 'Creating']:\n",
    "    minutes_passed += 0.5\n",
    "    endpt_status = client.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n",
    "    print(f'{minutes_passed} min: {endpt_status} endpoint; ',end='')\n",
    "    time.sleep(30)\n",
    "\n",
    "print('\\nFinished creating or updating endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate predictions from  the  test data set.\n",
    "\n",
    "Compared actual to predicted values of whether the transaction was Fraudulent (`1`) or not (`0`).  Then we'll produce a  confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    ret=csv.getvalue().decode().rstrip()\n",
    "    return ret\n",
    "\n",
    " \n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "#payload = np2csv(test_X)\n",
    "unlimited=100_000_000_000\n",
    "sample_len=unlimited\n",
    "\n",
    "\n",
    "def display(data):\n",
    "    print('Count', len(data), \n",
    "        '; Positives', sum(1 for y in data if y),\n",
    "      '; Negatives', sum(1 for y in data if not y))\n",
    "\n",
    "print('Test data: ', end='')\n",
    "display(test_y[:sample_len])\n",
    "    \n",
    "def batch_predict(data, batch_size):\n",
    "\n",
    "    def do_predict(data_):\n",
    "       payload = np2csv(data_)\n",
    "\n",
    "       response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=payload)\n",
    "       result = json.loads(response['Body'].read().decode())\n",
    "\n",
    "       test_pred = np.array([r['predicted_label'] for r in result['predictions']])\n",
    "       test_pred = [float(num) for num in test_pred]\n",
    "       assert all(t in [0.0, 1.0] for t in test_pred) , test_pred \n",
    "       test_pred_int = [round(num) for num in test_pred]\n",
    "       return test_pred_int\n",
    "\n",
    "    len_data = len(data)\n",
    "    arrs = []\n",
    "    \n",
    "    for offset in range(0, len_data, batch_size):\n",
    "        if offset + batch_size < len_data:\n",
    "            results = do_predict(data[offset:(offset+batch_size)])\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:len_data]))\n",
    "        sys.stdout.write('.')\n",
    "        if offset%(batch_size*100) == 0:\n",
    "            sys.stdout.write(f' ({offset}) ')\n",
    "    return(arrs)\n",
    "\n",
    "preds_ml = batch_predict(test_X[:sample_len], 1000)\n",
    "print('\\nPredictions: ', end='')\n",
    "display(preds_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ml = test_y[:sample_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_(labels_, predictions):\n",
    "   print('n:', len(labels_))\n",
    "   print('.', end='')\n",
    "    \n",
    "   assert len(labels_)==len(predictions)\n",
    "   \n",
    "   unmatched= sum(1 for i in range(len(predictions)) if predictions[i] != labels_[i])\n",
    "   matched = sum(1 for i in range(len(predictions)) if predictions[i] == labels_[i])\n",
    "\n",
    "   assert unmatched, 'Zero errors is very unexpected'\n",
    "   error = unmatched / float(len(predictions))\n",
    "   print(f'Prediction does not equal actual: {unmatched}')\n",
    "   print(f'Prediction equals actual: {matched}')\n",
    "\n",
    "   print(f'Error rate = {error:.8f}')\n",
    "  \n",
    "   actual_pos = sum(1 for i in range(len(labels_)) if 1 == labels_[i])\n",
    "   print('.', end='')\n",
    " \n",
    "   actual_neg = sum(1 for i in range(len(labels_)) if 0 == labels_[i])\n",
    "   print('.', end='')\n",
    "\n",
    "   pred_pos = sum(1 for i in range(len(predictions)) if 1 == predictions[i])\n",
    "   print('.', end='')\n",
    "\n",
    "   pred_neg = sum(1 for i in range(len(predictions)) if 0 == predictions[i])\n",
    "   print('.', end='')\n",
    "\n",
    "   true_pos = sum(1 for i in range(len(predictions)) if predictions[i] == 1 == labels_[i])\n",
    "   print('.', end='')\n",
    "\n",
    "   true_neg = sum(1 for i in range(len(predictions)) if predictions[i] == 0 == labels_[i])\n",
    "   print('.', end='')\n",
    "\n",
    "   false_pos=sum(1 for i in range(len(predictions)) if predictions[i] == 1 and 0 == labels_[i])\n",
    "   print('.', end='')\n",
    "\n",
    "   false_neg=sum(1 for i in range(len(predictions)) if predictions[i] == 0 and 1 == labels_[i])\n",
    "   print('.', end='')\n",
    "   \n",
    "   print('\\ntrue_pos', true_pos, 'false_pos', false_pos)\n",
    "   print('true_neg', true_neg, 'false_neg', false_neg)\n",
    "\n",
    "   assert  true_pos+false_pos == pred_pos\n",
    "  \n",
    "   recall = true_pos / (true_pos + false_neg)\n",
    "   print(f'Recall = {recall:.8f}')\n",
    "   precision = true_pos / (true_pos + false_pos)\n",
    "\n",
    "   print(f'Precision = {precision:.8f}')\n",
    "   if precision+recall == 0:\n",
    "       f1=float('inf')\n",
    "   else:\n",
    "       f1 = (2 * precision * recall) / (precision + recall)\n",
    "       \n",
    "   print(f'F1 = {f1:.2f}')\n",
    "\n",
    "\n",
    "   assert recall <= f1 <= precision or precision <= f1 <= recall  or precision==recall==0\n",
    "   assert true_pos + false_neg == actual_pos\n",
    "   assert true_neg + false_pos == actual_neg\n",
    "   assert len(predictions)==len(labels_)\n",
    "   assert actual_pos + actual_neg==len(labels_)\n",
    "   assert true_neg + false_neg== pred_neg\n",
    "   assert pred_pos + pred_neg==len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics')\n",
    "metrics_(labels_ml, preds_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=np.array(labels_ml), columns=np.array(preds_ml), rownames=['actual fraud'],  colnames=['predicted as fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up to save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.delete_endpoint(ll_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
